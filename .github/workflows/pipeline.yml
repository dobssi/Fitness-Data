# .github/workflows/pipeline.yml
# Pipeline — automated execution via GitHub Actions
#
# Triggers:
#   - Manual (workflow_dispatch) with mode selection
#   - Push to overrides file (auto-reprocesses after issue-based override)
#   - Scheduled daily run (uncomment cron when ready)
#
# Secrets required:
#   INTERVALS_API_KEY    — intervals.icu API key
#   INTERVALS_ATHLETE_ID — intervals.icu athlete ID
#   DROPBOX_TOKEN        — Dropbox API OAuth2 token (for cache + Master storage)

name: Run Pipeline

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Pipeline mode'
        type: choice
        options:
          - UPDATE
          - FULLPIPE
          - FORCE_FULL
          - FROM_MODEL
          - FROM_SIM
          - FROM_STEPB
          - DASHBOARD
          - CACHE
        default: UPDATE
      run_name:
        description: 'Run name (e.g. Haga parkrun)'
        type: string
        default: ''
      distance_km:
        description: 'Distance km (e.g. 5, 10, 21.097)'
        type: string
        default: ''
      run_date:
        description: 'Run date YYYY-MM-DD (default: today)'
        type: string
        default: ''
      run_number:
        description: 'Which run today? (1, 2, 3... blank=only)'
        type: string
        default: ''
      surface:
        description: 'Surface: TRAIL, TRACK, SNOW, HEAVY_SNOW'
        type: string
        default: ''
      surface_adj:
        description: 'Surface adj (e.g. 0.97 trail, 1.05 snow)'
        type: string
        default: ''
      notes:
        description: 'Notes'
        type: string
        default: ''
      race:
        description: 'Race?'
        type: boolean
        default: false
      parkrun:
        description: 'Parkrun?'
        type: boolean
        default: false
      sync:
        description: 'Sync from intervals.icu'
        type: boolean
        default: true
  
  push:
    paths:
      - 'activity_overrides.yml'
  
  # Uncomment for scheduled daily runs:
  # schedule:
  #   - cron: '0 20 * * *'  # Daily at 20:00 UTC (22:00 Stockholm)

# Prevent concurrent runs (latest wins)
concurrency:
  group: pipeline
  cancel-in-progress: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    
    env:
      INTERVALS_API_KEY: ${{ secrets.INTERVALS_API_KEY }}
      INTERVALS_ATHLETE_ID: ${{ secrets.INTERVALS_ATHLETE_ID }}
      DROPBOX_TOKEN: ${{ secrets.DROPBOX_TOKEN }}
      DROPBOX_REFRESH_TOKEN: ${{ secrets.DROPBOX_REFRESH_TOKEN }}
      DROPBOX_APP_KEY: ${{ secrets.DROPBOX_APP_KEY }}
      DROPBOX_APP_SECRET: ${{ secrets.DROPBOX_APP_SECRET }}
      PIPELINE_SIZE: FULL
    
    steps:
      # ─── Setup ──────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      # ─── Cache (GitHub Actions layer for speed) ─────────────
      - name: Restore persec cache
        uses: actions/cache@v4
        with:
          path: persec_cache_FULL
          key: persec-cache-${{ github.run_number }}
          restore-keys: |
            persec-cache-
      
      # ─── Download data from Dropbox ─────────────────────────
      - name: Download pipeline data from Dropbox
        run: |
          python ci/dropbox_sync.py download \
            --items Master_FULL_GPSQ_ID.xlsx \
                    Master_FULL_GPSQ_ID_simS4.xlsx \
                    Master_FULL_GPSQ_ID_post.xlsx \
                    athlete_data.csv \
                    activities.csv \
                    activity_overrides.xlsx \
                    Master_Rebuilt.xlsx \
                    re_model_s4_FULL.json \
                    sync_state.json \
                    fit_sync_state.json \
                    pending_activities.csv \
                    TotalHistory.zip \
                    _weather_cache_openmeteo/openmeteo_cache.sqlite
        continue-on-error: true
      
      # ─── Apply run metadata from dispatch inputs ────────────
      # Must run BEFORE sync so fetch_fit_files.py sees the dispatch
      # name in pending_activities.csv and skips auto-naming
      - name: Apply run metadata
        if: >-
          ${{ github.event.inputs.run_name != '' ||
              github.event.inputs.race == 'true' ||
              github.event.inputs.parkrun == 'true' ||
              github.event.inputs.distance_km != '' ||
              github.event.inputs.surface_adj != '' ||
              github.event.inputs.surface != '' ||
              github.event.inputs.notes != '' }}
        run: |
          ARGS=""
          if [ -n "${{ github.event.inputs.run_name }}" ]; then
            ARGS="$ARGS --run-name \"${{ github.event.inputs.run_name }}\""
          fi
          if [ "${{ github.event.inputs.race }}" = "true" ]; then
            ARGS="$ARGS --race"
          fi
          if [ "${{ github.event.inputs.parkrun }}" = "true" ]; then
            ARGS="$ARGS --parkrun"
          fi
          if [ -n "${{ github.event.inputs.distance_km }}" ]; then
            ARGS="$ARGS --distance ${{ github.event.inputs.distance_km }}"
          fi
          if [ -n "${{ github.event.inputs.surface_adj }}" ]; then
            ARGS="$ARGS --surface-adj ${{ github.event.inputs.surface_adj }}"
          fi
          if [ -n "${{ github.event.inputs.surface }}" ]; then
            ARGS="$ARGS --surface ${{ github.event.inputs.surface }}"
          fi
          if [ -n "${{ github.event.inputs.run_date }}" ]; then
            ARGS="$ARGS --date ${{ github.event.inputs.run_date }}"
          fi
          if [ -n "${{ github.event.inputs.run_number }}" ]; then
            ARGS="$ARGS --run-number ${{ github.event.inputs.run_number }}"
          fi
          if [ -n "${{ github.event.inputs.notes }}" ]; then
            ARGS="$ARGS --notes \"${{ github.event.inputs.notes }}\""
          fi
          eval python ci/apply_run_metadata.py $ARGS

      # Upload overrides immediately so queued runs see changes
      - name: Save metadata to Dropbox
        if: >-
          ${{ github.event.inputs.run_name != '' ||
              github.event.inputs.race == 'true' ||
              github.event.inputs.parkrun == 'true' ||
              github.event.inputs.distance_km != '' ||
              github.event.inputs.surface_adj != '' ||
              github.event.inputs.surface != '' ||
              github.event.inputs.notes != '' }}
        run: |
          python ci/dropbox_sync.py upload \
            --items activity_overrides.xlsx \
                    pending_activities.csv
        continue-on-error: true

      # ─── Sync from intervals.icu ────────────────────────────
      - name: Sync from intervals.icu
        if: ${{ github.event.inputs.sync != 'false' }}
        run: |
          python fetch_fit_files.py \
            --fit-dir TotalHistory \
            --zip TotalHistory.zip \
            --state-file sync_state.json
          
          python sync_athlete_data.py \
            --athlete-data athlete_data.csv
      
      # ─── Run pipeline ──────────────────────────────────────
      - name: Run pipeline
        run: |
          MODE="${{ github.event.inputs.mode || 'UPDATE' }}"
          python run_pipeline.py $MODE \
            --size FULL \
            --ci \
            --skip-push
      
      # ─── Generate dashboard ─────────────────────────────────
      - name: Generate dashboard
        run: python generate_dashboard.py
        continue-on-error: true
      
      # ─── Upload results to Dropbox ─────────────────────────
      - name: Upload results to Dropbox
        run: |
          python ci/dropbox_sync.py upload \
            --items Master_FULL_GPSQ_ID.xlsx \
                    Master_FULL_GPSQ_ID_simS4.xlsx \
                    Master_FULL_GPSQ_ID_post.xlsx \
                    athlete_data.csv \
                    re_model_s4_FULL.json \
                    sync_state.json \
                    activity_overrides.xlsx \
                    index.html \
                    _weather_cache_openmeteo/openmeteo_cache.sqlite \
                    TotalHistory.zip \
                    fit_sync_state.json \
                    pending_activities.csv
      
      # ─── Deploy dashboard to GitHub Pages ──────────────────
      - name: Prepare dashboard for Pages
        run: |
          mkdir -p docs
          cp -f index.html docs/index.html 2>/dev/null || true
        continue-on-error: true
      
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        if: success()
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
          publish_branch: gh-pages
        continue-on-error: true
      
      # ─── Summary ────────────────────────────────────────────
      - name: Summary
        if: always()
        run: |
          echo "## Pipeline Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Mode:** ${{ github.event.inputs.mode || 'UPDATE' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Time:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_STEP_SUMMARY
          
          if [ -f Master_FULL_GPSQ_ID_post.xlsx ]; then
            SIZE=$(stat -c%s Master_FULL_GPSQ_ID_post.xlsx 2>/dev/null || echo "?")
            echo "- **Master size:** $SIZE bytes" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -d persec_cache_FULL ]; then
            COUNT=$(find persec_cache_FULL -name "*.npz" | wc -l)
            echo "- **Cache files:** $COUNT .npz" >> $GITHUB_STEP_SUMMARY
          fi
